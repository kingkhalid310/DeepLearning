{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Work 2 \n",
    "### Part 2 (Implementation Questions)\n",
    "Name: Khalid Saifullah \\\n",
    "ID: A20423546 \\\n",
    "Semester: Spring 2021 \\\n",
    "Date: 17th March 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Comment or uncomment this cell to run the iris dataset!!!######\n",
    "\n",
    "\n",
    "#Loading the iris dataset\n",
    "iris = pd.read_csv(\"Iris.csv\")\n",
    "iris = iris.sample(frac=1).reset_index(drop=True) # Shuffle\n",
    "\n",
    "# Separating independent and dependent variables\n",
    "X = np.array(iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]) #Ignored the ID column\n",
    "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) #min-max normalisation used for the features\n",
    "Y = iris.Species #Only selected the last column i.e. the species column\n",
    "one_hot_encoder = OneHotEncoder(sparse=False) #Shuffled to remove any biases due to ordering\n",
    "Y = one_hot_encoder.fit_transform(np.array(Y).reshape(-1, 1)) #Returns one hot encoded array of the 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Comment or uncomment this cell to run the wine dataset!!!######\n",
    "\n",
    "\n",
    "#Loading the wine dataset\n",
    "#wine = pd.read_csv(\"wine.csv\")\n",
    "#wine = wine.sample(frac=1).reset_index(drop=True) # Shuffle\n",
    "\n",
    "#Searching for any missing values in the dataset\n",
    "#wine.isnull().sum() #No missing values found - preproscessing step\n",
    "\n",
    "# Separating independent and dependent variables\n",
    "#X = wine.drop('Types',axis=1)\n",
    "#X = np.array(wine[['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']])\n",
    "#X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) #min-max normalisation used for the features\n",
    "#Y = wine['Types']\n",
    "#one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "#Y = one_hot_encoder.fit_transform(np.array(Y).reshape(-1, 1)) #Returns one hot encoded array of the 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitted our dataset for training, testing and validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"nodes\" is a list of integers. Each integer denotes the number of nodes in each layer. \n",
    "#The length of this list denotes the number of layers.\n",
    "\n",
    "def NeuralNetwork(X_train, Y_train, X_val=None, Y_val=None, epochs=10, nodes=[], lr=0.15): #epochs by default set to 10\n",
    "    hidden_layers = len(nodes) - 1 #Subtracting 1 becaues of the output layer which is included in the len(nodes) value\n",
    "    weights = InitializeWeights(nodes) #initiasing the weights in all the nodes\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        weights = Train(X_train, Y_train, lr, weights)\n",
    "        #printing testing accuracies every 10 epochs\n",
    "        if(epoch % 10 == 0):\n",
    "            print(\"Epoch: {}\".format(epoch))\n",
    "            print(\"Training Accuracy: {}\".format(Accuracy(X_train, Y_train, weights)))\n",
    "            if X_val.any():\n",
    "                print(\"Validation Accuracy: {}\".format(Accuracy(X_val, Y_val, weights)))\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each element in the weights list represents a hidden layer and holds the weights \n",
    "#of connections from the previous layer (including the bias) to the current layer.\n",
    "#So, element i in weights holds the weights of the connections from layer i-1 to layer i.\n",
    "#Note that the input layer has no incoming connections so it is not present in weights.\n",
    "\n",
    "#Initialising weights with random values in the range -1 to 1.\n",
    "def InitializeWeights(nodes):\n",
    "    layers, weights = len(nodes), []\n",
    "    \n",
    "    for i in range(1, layers):\n",
    "        w = [[np.random.uniform(-1, 1) for k in range(nodes[i-1] + 1)] #Try using: w = [[np.random.normal(-1, 1) for k in range(nodes[i-1] + 1)]\n",
    "              for j in range(nodes[i])]\n",
    "        weights.append(np.matrix(w))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward propagation\n",
    "def ForwardPropagation(x, weights, layers):\n",
    "    activations, layer_input = [x], x\n",
    "    for j in range(layers):\n",
    "        activation_out = Sigmoid(np.dot(layer_input, weights[j].T))\n",
    "        activations.append(activation_out)\n",
    "        layer_input = np.append(1, activation_out) # Augmenting with the bias term\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward propagation\n",
    "def BackPropagation(y, activations, weights, layers):\n",
    "    outputFinal = activations[-1]\n",
    "    error = np.matrix(y - outputFinal) # Error at output\n",
    "    \n",
    "    for j in range(layers, 0, -1): #increments by -1 i.e. we are traversing from the last layer\n",
    "        currentActivation = activations[j]\n",
    "        \n",
    "        if(j > 1):\n",
    "            # Augmenting previous activation\n",
    "            previousActivation = np.append(1, activations[j-1])\n",
    "        else:\n",
    "            # First hidden layer, prevActivation is input (without bias)\n",
    "            previousActivation = activations[0]\n",
    "        \n",
    "        del_value = np.multiply(error, SigmoidDerivative(currentActivation)) #calculating the delta value\n",
    "        weights[j-1] += lr * np.multiply(del_value.T, previousActivation) #updating the weights\n",
    "\n",
    "        w = np.delete(weights[j-1], [0], axis=1) # Removing bias from weights\n",
    "        error = np.dot(del_value, w) # Calculate error for current layer\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training our model\n",
    "def Train(X, Y, lr, weights):\n",
    "    layers = len(weights)\n",
    "    for i in range(len(X)):\n",
    "        x, y = X[i], Y[i]\n",
    "        x = np.matrix(np.append(1, x)) # Augmenting the feature vector\n",
    "        \n",
    "        activations = ForwardPropagation(x, weights, layers)\n",
    "        weights = BackPropagation(y, activations, weights, layers)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the sigmoid activation function and its derivative\n",
    "def Sigmoid(x):\n",
    "    return expit(x)\n",
    "\n",
    "def SigmoidDerivative(x):\n",
    "    return np.multiply(x, 1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting here\n",
    "def Predict(sample, weights):\n",
    "    layers = len(weights)\n",
    "    sample = np.append(1, sample) # Augmenting the feature vector\n",
    "    \n",
    "    #Forward Propagation\n",
    "    activations = ForwardPropagation(sample, weights, layers)\n",
    "    \n",
    "    outputFinal = activations[-1].A1\n",
    "    index = FindMaxActivation(outputFinal)\n",
    "\n",
    "    #Initializing our prediction vector to zero\n",
    "    y = [0 for i in range(len(outputFinal))]\n",
    "    y[index] = 1  # Set guessed class to 1\n",
    "\n",
    "    return y # Return the prediction vector\n",
    "\n",
    "\n",
    "#Finding the most probable class\n",
    "def FindMaxActivation(output):\n",
    "    m, index = output[0], 0\n",
    "    for i in range(1, len(output)):\n",
    "        if(output[i] > m):\n",
    "            m, index = output[i], i\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the model accuracy\n",
    "def Accuracy(X, Y, weights):\n",
    "    \"\"\"Run set through network, find overall accuracy\"\"\"\n",
    "    score = 0 #initialising our scorecard\n",
    "    for i in range(len(X)):\n",
    "        OurPrediction = Predict(X[i], weights)\n",
    "\n",
    "        if(list(Y[i]) == OurPrediction):\n",
    "            score += 1 # correctly predicted\n",
    "\n",
    "    return score / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Training Accuracy: 0.3888888888888889\n",
      "Validation Accuracy: 0.2222222222222222\n",
      "Epoch: 20\n",
      "Training Accuracy: 0.37962962962962965\n",
      "Validation Accuracy: 0.2222222222222222\n",
      "Epoch: 30\n",
      "Training Accuracy: 0.3611111111111111\n",
      "Validation Accuracy: 0.2222222222222222\n",
      "Epoch: 40\n",
      "Training Accuracy: 0.6574074074074074\n",
      "Validation Accuracy: 0.7777777777777778\n",
      "Epoch: 50\n",
      "Training Accuracy: 0.7407407407407407\n",
      "Validation Accuracy: 0.7777777777777778\n",
      "Epoch: 60\n",
      "Training Accuracy: 0.8518518518518519\n",
      "Validation Accuracy: 0.8888888888888888\n",
      "Epoch: 70\n",
      "Training Accuracy: 0.9351851851851852\n",
      "Validation Accuracy: 0.9629629629629629\n",
      "Epoch: 80\n",
      "Training Accuracy: 0.9629629629629629\n",
      "Validation Accuracy: 0.9629629629629629\n",
      "Epoch: 90\n",
      "Training Accuracy: 0.9814814814814815\n",
      "Validation Accuracy: 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "f = len(X[0]) # Number of features\n",
    "o = len(Y[0]) # Number of outputs or classes\n",
    "\n",
    "#Change the hidden nodes, learning rates and epochs below to fine tune the model hyperparameters\n",
    "#Use hiddenlayer1=4 and hiddenlayer=8 for iris dataset\n",
    "#Use hiddenlayer1=8 and hiddenlayer=8 for iris dataset\n",
    "hiddenlayer1 = 4\n",
    "hiddenlayer2 = 8\n",
    "layers = [f, hiddenlayer1, hiddenlayer2, o] # Number of nodes in layers. Note the two hidden layers in the middle\n",
    "lr, epochs = 0.1, 90\n",
    "\n",
    "weights = NeuralNetwork(X_train, Y_train, X_val, Y_val, epochs=epochs, nodes=layers, lr=lr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy: {}\".format(Accuracy(X_test, Y_test, weights)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
